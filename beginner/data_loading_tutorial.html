

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Data Loading and Processing Tutorial &mdash; PyTorch Tutorials 0.4.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/pytorch_theme.css" type="text/css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning for NLP with Pytorch" href="deep_learning_nlp_tutorial.html" />
    <link rel="prev" title="Transfer Learning tutorial" href="transfer_learning_tutorial.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <!-- <link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i" rel="stylesheet"> -->
</head>

<div class="pytorch-header">
  <div class="pytorch-container">
    <div class="pytorch-header-logo">
      <img src="../_static/pytorch-logo-dark.svg" class="logo" alt="Logo"/>
    </div>

    <div class="pytorch-main-menu">
      <ul>
        <li><a href="">Get Started</a></li>
        <li><a href="">Features</a></li>
        <li><a href="">Ecosystem</a></li>
        <li><a href="">Blog</a></li>
        <li><a href="" class="active">Tutorials</a></li>
        <li><a href="">Docs</a></li>
        <li><a href="">Resources</a></li>
        <li><a href="">Github</a></li>
      </ul>
    </div>
  </div>
</div>

<body class="pytorch-body">

   
  <div>

    
    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-left-menu-search">
          

          
            
            
              <div class="version">
                0.4.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="former_torchies_tutorial.html">PyTorch for former Torch users</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning_tutorial.html">Transfer Learning tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Data Loading and Processing Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dataset-class">Dataset class</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transforms">Transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#compose-transforms">Compose transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#iterating-through-the-dataset">Iterating through the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#afterword-torchvision">Afterword: torchvision</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a></li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/char_rnn_generation_tutorial.html">Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/seq2seq_translation_tutorial.html">Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/neural_style_tutorial.html">Neural Transfer with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Data Loading and Processing Tutorial</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/beginner/data_loading_tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
</div>
          <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
           <article itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-beginner-data-loading-tutorial-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="data-loading-and-processing-tutorial">
<span id="sphx-glr-beginner-data-loading-tutorial-py"></span><h1>Data Loading and Processing Tutorial<a class="headerlink" href="#data-loading-and-processing-tutorial" title="Permalink to this headline">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://chsasank.github.io">Sasank Chilamkurthy</a></p>
<p>A lot of effort in solving any machine learning problem goes in to
preparing the data. PyTorch provides many tools to make data loading
easy and hopefully, to make your code more readable. In this tutorial,
we will see how to load and preprocess/augment data from a non trivial
dataset.</p>
<p>To run this tutorial, please make sure the following packages are
installed:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">scikit-image</span></code>: For image io and transforms</li>
<li><code class="docutils literal notranslate"><span class="pre">pandas</span></code>: For easier csv parsing</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">io</span><span class="p">,</span> <span class="n">transform</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Ignore warnings</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>   <span class="c1"># interactive mode</span>
</pre></div>
</div>
<p>The dataset we are going to deal with is that of facial pose.
This means that a face is annotated like this:</p>
<div class="figure">
<a class="reference internal image-reference" href="../_images/landmarked_face2.png"><img alt="../_images/landmarked_face2.png" src="../_images/landmarked_face2.png" style="width: 400px;" /></a>
</div>
<p>Over all, 68 different landmark points are annotated for each face.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Download the dataset from <a class="reference external" href="https://download.pytorch.org/tutorial/faces.zip">here</a>
so that the images are in a directory named ‘faces/’.
This dataset was actually
generated by applying excellent <a class="reference external" href="http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html">dlib’s pose
estimation</a>
on a few images from imagenet tagged as ‘face’.</p>
</div>
<p>Dataset comes with a csv file with annotations which looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">image_name</span><span class="p">,</span><span class="n">part_0_x</span><span class="p">,</span><span class="n">part_0_y</span><span class="p">,</span><span class="n">part_1_x</span><span class="p">,</span><span class="n">part_1_y</span><span class="p">,</span><span class="n">part_2_x</span><span class="p">,</span> <span class="o">...</span> <span class="p">,</span><span class="n">part_67_x</span><span class="p">,</span><span class="n">part_67_y</span>
<span class="mi">0805</span><span class="n">personali01</span><span class="o">.</span><span class="n">jpg</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">83</span><span class="p">,</span><span class="mi">27</span><span class="p">,</span><span class="mi">98</span><span class="p">,</span> <span class="o">...</span> <span class="mi">84</span><span class="p">,</span><span class="mi">134</span>
<span class="mi">1084239450</span><span class="n">_e76e00b7e7</span><span class="o">.</span><span class="n">jpg</span><span class="p">,</span><span class="mi">70</span><span class="p">,</span><span class="mi">236</span><span class="p">,</span><span class="mi">71</span><span class="p">,</span><span class="mi">257</span><span class="p">,</span> <span class="o">...</span> <span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">312</span>
</pre></div>
</div>
<p>Let’s quickly read the CSV and get the annotations in an (N, 2) array where N
is the number of landmarks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">landmarks_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;faces/face_landmarks.csv&#39;</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">65</span>
<span class="n">img_name</span> <span class="o">=</span> <span class="n">landmarks_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">landmarks</span> <span class="o">=</span> <span class="n">landmarks_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
<span class="n">landmarks</span> <span class="o">=</span> <span class="n">landmarks</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Image name: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">img_name</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Landmarks shape: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">landmarks</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;First 4 Landmarks: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">landmarks</span><span class="p">[:</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Image name: person-7.jpg
Landmarks shape: (68, 2)
First 4 Landmarks: [[32. 65.]
 [33. 76.]
 [34. 86.]
 [34. 97.]]
</pre></div>
</div>
<p>Let’s write a simple helper function to show an image and its landmarks
and use it to show a sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">show_landmarks</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">landmarks</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Show image with landmarks&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">landmarks</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">landmarks</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># pause a bit so that plots are updated</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">show_landmarks</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;faces/&#39;</span><span class="p">,</span> <span class="n">img_name</span><span class="p">)),</span>
               <span class="n">landmarks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/sphx_glr_data_loading_tutorial_001.png" class="sphx-glr-single-img" src="../_images/sphx_glr_data_loading_tutorial_001.png" />
<div class="section" id="dataset-class">
<h2>Dataset class<a class="headerlink" href="#dataset-class" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code> is an abstract class representing a
dataset.
Your custom dataset should inherit <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and override the following
methods:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">__len__</span></code> so that <code class="docutils literal notranslate"><span class="pre">len(dataset)</span></code> returns the size of the dataset.</li>
<li><code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> to support the indexing such that <code class="docutils literal notranslate"><span class="pre">dataset[i]</span></code> can
be used to get <span class="math notranslate nohighlight">\(i\)</span>th sample</li>
</ul>
<p>Let’s create a dataset class for our face landmarks dataset. We will
read the csv in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> but leave the reading of images to
<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>. This is memory efficient because all the images are not
stored in the memory at once but read as required.</p>
<p>Sample of our dataset will be a dict
<code class="docutils literal notranslate"><span class="pre">{'image':</span> <span class="pre">image,</span> <span class="pre">'landmarks':</span> <span class="pre">landmarks}</span></code>. Our datset will take an
optional argument <code class="docutils literal notranslate"><span class="pre">transform</span></code> so that any required processing can be
applied on the sample. We will see the usefulness of <code class="docutils literal notranslate"><span class="pre">transform</span></code> in the
next section.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FaceLandmarksDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">root_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            csv_file (string): Path to the csv file with annotations.</span>
<span class="sd">            root_dir (string): Directory with all the images.</span>
<span class="sd">            transform (callable, optional): Optional transform to be applied</span>
<span class="sd">                on a sample.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span> <span class="o">=</span> <span class="n">root_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_dir</span><span class="p">,</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_name</span><span class="p">)</span>
        <span class="n">landmarks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">landmarks_frame</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">()</span>
        <span class="n">landmarks</span> <span class="o">=</span> <span class="n">landmarks</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">image</span><span class="p">,</span> <span class="s1">&#39;landmarks&#39;</span><span class="p">:</span> <span class="n">landmarks</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sample</span>
</pre></div>
</div>
<p>Let’s instantiate this class and iterate through the data samples. We
will print the sizes of first 4 samples and show their landmarks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">face_dataset</span> <span class="o">=</span> <span class="n">FaceLandmarksDataset</span><span class="p">(</span><span class="n">csv_file</span><span class="o">=</span><span class="s1">&#39;faces/face_landmarks.csv&#39;</span><span class="p">,</span>
                                    <span class="n">root_dir</span><span class="o">=</span><span class="s1">&#39;faces/&#39;</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">face_dataset</span><span class="p">)):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">face_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sample #{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">show_landmarks</span><span class="p">(</span><span class="o">**</span><span class="n">sample</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">break</span>
</pre></div>
</div>
<img alt="../_images/sphx_glr_data_loading_tutorial_002.png" class="sphx-glr-single-img" src="../_images/sphx_glr_data_loading_tutorial_002.png" />
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 (324, 215, 3) (68, 2)
1 (500, 333, 3) (68, 2)
2 (250, 258, 3) (68, 2)
3 (434, 290, 3) (68, 2)
</pre></div>
</div>
</div>
<div class="section" id="transforms">
<h2>Transforms<a class="headerlink" href="#transforms" title="Permalink to this headline">¶</a></h2>
<p>One issue we can see from the above is that the samples are not of the
same size. Most neural networks expect the images of a fixed size.
Therefore, we will need to write some prepocessing code.
Let’s create three transforms:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">Rescale</span></code>: to scale the image</li>
<li><code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code>: to crop from image randomly. This is data
augmentation.</li>
<li><code class="docutils literal notranslate"><span class="pre">ToTensor</span></code>: to convert the numpy images to torch images (we need to
swap axes).</li>
</ul>
<p>We will write them as callable classes instead of simple functions so
that parameters of the transform need not be passed everytime it’s
called. For this, we just need to implement <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method and
if required, <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. We can then use a transform like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tsfm</span> <span class="o">=</span> <span class="n">Transform</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">transformed_sample</span> <span class="o">=</span> <span class="n">tsfm</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
<p>Observe below how these transforms had to be applied both on the image and
landmarks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Rescale</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Rescale the image in a sample to a given size.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (tuple or int): Desired output size. If tuple, output is</span>
<span class="sd">            matched to output_size. If int, smaller of image edges is matched</span>
<span class="sd">            to output_size keeping aspect ratio the same.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">landmarks</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span>

        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">&gt;</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">/</span> <span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">*</span> <span class="n">w</span> <span class="o">/</span> <span class="n">h</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span>

        <span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">new_h</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">new_w</span><span class="p">)</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">transform</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span><span class="p">))</span>

        <span class="c1"># h and w are swapped for landmarks because for images,</span>
        <span class="c1"># x and y axes are axis 1 and 0 respectively</span>
        <span class="n">landmarks</span> <span class="o">=</span> <span class="n">landmarks</span> <span class="o">*</span> <span class="p">[</span><span class="n">new_w</span> <span class="o">/</span> <span class="n">w</span><span class="p">,</span> <span class="n">new_h</span> <span class="o">/</span> <span class="n">h</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">img</span><span class="p">,</span> <span class="s1">&#39;landmarks&#39;</span><span class="p">:</span> <span class="n">landmarks</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">RandomCrop</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Crop randomly the image in a sample.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (tuple or int): Desired output size. If int, square crop</span>
<span class="sd">            is made.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">landmarks</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span>

        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">new_h</span><span class="p">,</span> <span class="n">new_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span>

        <span class="n">top</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">h</span> <span class="o">-</span> <span class="n">new_h</span><span class="p">)</span>
        <span class="n">left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="n">new_w</span><span class="p">)</span>

        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="n">top</span><span class="p">:</span> <span class="n">top</span> <span class="o">+</span> <span class="n">new_h</span><span class="p">,</span>
                      <span class="n">left</span><span class="p">:</span> <span class="n">left</span> <span class="o">+</span> <span class="n">new_w</span><span class="p">]</span>

        <span class="n">landmarks</span> <span class="o">=</span> <span class="n">landmarks</span> <span class="o">-</span> <span class="p">[</span><span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">]</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">image</span><span class="p">,</span> <span class="s1">&#39;landmarks&#39;</span><span class="p">:</span> <span class="n">landmarks</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">ToTensor</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert ndarrays in sample to Tensors.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">landmarks</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span>

        <span class="c1"># swap color axis because</span>
        <span class="c1"># numpy image: H x W x C</span>
        <span class="c1"># torch image: C X H X W</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">image</span><span class="p">),</span>
                <span class="s1">&#39;landmarks&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">landmarks</span><span class="p">)}</span>
</pre></div>
</div>
<div class="section" id="compose-transforms">
<h3>Compose transforms<a class="headerlink" href="#compose-transforms" title="Permalink to this headline">¶</a></h3>
<p>Now, we apply the transforms on an sample.</p>
<p>Let’s say we want to rescale the shorter side of the image to 256 and
then randomly crop a square of size 224 from it. i.e, we want to compose
<code class="docutils literal notranslate"><span class="pre">Rescale</span></code> and <code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code> transforms.
<code class="docutils literal notranslate"><span class="pre">torchvision.transforms.Compose</span></code> is a simple callable class which allows us
to do this.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scale</span> <span class="o">=</span> <span class="n">Rescale</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="n">crop</span> <span class="o">=</span> <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
<span class="n">composed</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">Rescale</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
                               <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">)])</span>

<span class="c1"># Apply each of the above transforms on sample.</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">face_dataset</span><span class="p">[</span><span class="mi">65</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tsfrm</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">scale</span><span class="p">,</span> <span class="n">crop</span><span class="p">,</span> <span class="n">composed</span><span class="p">]):</span>
    <span class="n">transformed_sample</span> <span class="o">=</span> <span class="n">tsfrm</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tsfrm</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">show_landmarks</span><span class="p">(</span><span class="o">**</span><span class="n">transformed_sample</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/sphx_glr_data_loading_tutorial_003.png" class="sphx-glr-single-img" src="../_images/sphx_glr_data_loading_tutorial_003.png" />
</div>
</div>
<div class="section" id="iterating-through-the-dataset">
<h2>Iterating through the dataset<a class="headerlink" href="#iterating-through-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>Let’s put this all together to create a dataset with composed
transforms.
To summarize, every time this dataset is sampled:</p>
<ul class="simple">
<li>An image is read from the file on the fly</li>
<li>Transforms are applied on the read image</li>
<li>Since one of the transforms is random, data is augmentated on
sampling</li>
</ul>
<p>We can iterate over the created dataset with a <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range</span></code>
loop as before.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">transformed_dataset</span> <span class="o">=</span> <span class="n">FaceLandmarksDataset</span><span class="p">(</span><span class="n">csv_file</span><span class="o">=</span><span class="s1">&#39;faces/face_landmarks.csv&#39;</span><span class="p">,</span>
                                           <span class="n">root_dir</span><span class="o">=</span><span class="s1">&#39;faces/&#39;</span><span class="p">,</span>
                                           <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                               <span class="n">Rescale</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
                                               <span class="n">RandomCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                               <span class="n">ToTensor</span><span class="p">()</span>
                                           <span class="p">]))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformed_dataset</span><span class="p">)):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">transformed_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 torch.Size([3, 224, 224]) torch.Size([68, 2])
1 torch.Size([3, 224, 224]) torch.Size([68, 2])
2 torch.Size([3, 224, 224]) torch.Size([68, 2])
3 torch.Size([3, 224, 224]) torch.Size([68, 2])
</pre></div>
</div>
<p>However, we are losing a lot of features by using a simple <code class="docutils literal notranslate"><span class="pre">for</span></code> loop to
iterate over the data. In particular, we are missing out on:</p>
<ul class="simple">
<li>Batching the data</li>
<li>Shuffling the data</li>
<li>Load the data in parallel using <code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> workers.</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> is an iterator which provides all these
features. Parameters used below should be clear. One parameter of
interest is <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code>. You can specify how exactly the samples need
to be batched using <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code>. However, default collate should work
fine for most use cases.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">transformed_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>


<span class="c1"># Helper function to show a batch</span>
<span class="k">def</span> <span class="nf">show_landmarks_batch</span><span class="p">(</span><span class="n">sample_batched</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Show image with landmarks for a batch of samples.&quot;&quot;&quot;</span>
    <span class="n">images_batch</span><span class="p">,</span> <span class="n">landmarks_batch</span> <span class="o">=</span> \
            <span class="n">sample_batched</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">],</span> <span class="n">sample_batched</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">images_batch</span><span class="p">)</span>
    <span class="n">im_size</span> <span class="o">=</span> <span class="n">images_batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">images_batch</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">landmarks_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">im_size</span><span class="p">,</span>
                    <span class="n">landmarks_batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                    <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Batch from dataloader&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i_batch</span><span class="p">,</span> <span class="n">sample_batched</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i_batch</span><span class="p">,</span> <span class="n">sample_batched</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
          <span class="n">sample_batched</span><span class="p">[</span><span class="s1">&#39;landmarks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

    <span class="c1"># observe 4th batch and stop.</span>
    <span class="k">if</span> <span class="n">i_batch</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
        <span class="n">show_landmarks_batch</span><span class="p">(</span><span class="n">sample_batched</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">break</span>
</pre></div>
</div>
<img alt="../_images/sphx_glr_data_loading_tutorial_004.png" class="sphx-glr-single-img" src="../_images/sphx_glr_data_loading_tutorial_004.png" />
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])
1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])
2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])
3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])
</pre></div>
</div>
</div>
<div class="section" id="afterword-torchvision">
<h2>Afterword: torchvision<a class="headerlink" href="#afterword-torchvision" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we have seen how to write and use datasets, transforms
and dataloader. <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> package provides some common datasets and
transforms. You might not even have to write custom classes. One of the
more generic datasets available in torchvision is <code class="docutils literal notranslate"><span class="pre">ImageFolder</span></code>.
It assumes that images are organized in the following way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">ants</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">ants</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">jpeg</span>
<span class="n">root</span><span class="o">/</span><span class="n">ants</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">png</span>
<span class="o">.</span>
<span class="o">.</span>
<span class="o">.</span>
<span class="n">root</span><span class="o">/</span><span class="n">bees</span><span class="o">/</span><span class="mf">123.</span><span class="n">jpg</span>
<span class="n">root</span><span class="o">/</span><span class="n">bees</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">bees</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<p>where ‘ants’, ‘bees’ etc. are class labels. Similarly generic transforms
which operate on <code class="docutils literal notranslate"><span class="pre">PIL.Image</span></code> like  <code class="docutils literal notranslate"><span class="pre">RandomHorizontalFlip</span></code>, <code class="docutils literal notranslate"><span class="pre">Scale</span></code>,
are also avaiable. You can use these to write a dataloader like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="k">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomSizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                             <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
    <span class="p">])</span>
<span class="n">hymenoptera_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;hymenoptera_data/train&#39;</span><span class="p">,</span>
                                           <span class="n">transform</span><span class="o">=</span><span class="n">data_transform</span><span class="p">)</span>
<span class="n">dataset_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">hymenoptera_dataset</span><span class="p">,</span>
                                             <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                             <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>For an example with training code, please see
<a class="reference internal" href="transfer_learning_tutorial.html"><span class="doc">Transfer Learning tutorial</span></a>.</p>
<p><strong>Total running time of the script:</strong> ( 0 minutes  38.223 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-data-loading-tutorial-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../_downloads/data_loading_tutorial.py" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">data_loading_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../_downloads/data_loading_tutorial.ipynb" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">data_loading_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </article>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="deep_learning_nlp_tutorial.html" class="btn btn-neutral float-right" title="Deep Learning for NLP with Pytorch" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="transfer_learning_tutorial.html" class="btn btn-neutral" title="Transfer Learning tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

      <div class="pytorch-content-right">
        <div class="pytorch-right-menu">
        </div>
      </div>
    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.4.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 

<script type="text/javascript">
  $(document).ready(function() {
    console.log('Testing...');
  });
</script>


</body>
</html>